# -*- coding: utf-8 -*-
"""Word Count Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZGv38-rlSs9wLzjJqt_TSnRRRib_B7_P
"""

# Commented out IPython magic to ensure Python compatibility.
# %fs ls /FileStore/shared_uploads/sample.json

df=spark.read.option("multiline", "true").json("dbfs:/FileStore/shared_uploads/sample.json")

display(df)

import re

review_rdd = df.select('review_detail').rdd
print(review_rdd.take(2)) #print first two row

words_rdd = review_rdd.flatMap(lambda review:re.split("\W+", str(review[0]).lower()))
print(words_rdd.take(5))

words_filter = words_rdd.filter(lambda word:len(word) >4)
print(words_filter.take(5))

words_mapped = words_filter.map(lambda word: (word, 1))
words_reduced = words_mapped.reduceByKey(lambda x,y: x+y)
words_sorted = words_reduced.map(lambda wc: (wc[1],wc[0])).sortByKey(False).map(lambda cw: (cw[1], cw[0]))

top_words = words_sorted.take(10)

for word in top_words:
    print(word)